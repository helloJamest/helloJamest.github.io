<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>机器学习基础概念 | Jamest</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习基础概念</h1><a id="logo" href="/.">Jamest</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习基础概念</h1><div class="post-meta">Apr 23, 2019</div><div class="post-content"><p>记录一些常见的机器学习基础概念。<br><a id="more"></a></p>
<h2 id="常见的距离算法"><a href="#常见的距离算法" class="headerlink" title="常见的距离算法"></a>常见的距离算法</h2><ol>
<li><p>欧几里得距离（Euclidean Distance）<br>$$<br>\sqrt{\sum_{i=1}^N{(x_i-y_i)}^2}<br>$$<br>标准欧氏距离的思路：现将各个维度的数据进行标准化：标准化后的值 = ( 标准化前的值 － 分量的均值 ) /分量的标准差，然后计算欧式距离</p>
</li>
<li><p>马哈拉诺比斯距离（Mahalanobis Distance）<br>若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离；如果去掉马氏距离中的协方差矩阵，就退化为欧氏距离。欧式距离就好比一个参照值，它表征的是当所有类别等概率出现的情况下，类别之间的距离；当类别先验概率并不相等时，马氏距离中引入的协方差参数（表征的是点的稀密程度）来平衡两个类别的概率。</p>
</li>
<li><p>曼哈顿距离（Manhattan Distance）<br>$$<br>\sum_{k=1}^n{|x_{1k}-x_{2k}|}<br>$$</p>
</li>
<li><p>海明距离（Hamming distance）<br>定义：在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。场景：在海量物品的相似度计算中可用simHash对物品压缩成字符串，然后使用海明距离计算物品间的距离</p>
</li>
</ol>
<h2 id="协方差与相关系数"><a href="#协方差与相关系数" class="headerlink" title="协方差与相关系数"></a>协方差与相关系数</h2><ul>
<li><p>协方差表示的是两个变量的总体的误差，范围负无穷到正无穷。<br>$$<br>cov(X,Y)=E[(X-μ_x)(Y-μ_y)]<br>$$<br>它反映了两个变量远离均值的过程是同方向变化还是反方向变化，是正相关还是负相关。协方差数值越大，相关程度越高。如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。</p>
</li>
<li><p>相关系数用来度量两个变量间的线性关系。范围-1到+1。<br>$$<br>ρ=\frac{cov{(X,Y)}}{σ_X σ_Y}<br>$$<br>用X、Y的协方差除以X的标准差和Y的标准差。 <strong>可以将相关系数看成一种特殊的协方差，是一种剔除了两个变量量纲影响、标准化后的特殊协方差，它消除了两个变量变化幅度的影响，而只是单纯反应变量间变化的相似程度。</strong><br>（即，标准化后的两个数据的相关系数等于其协方差）</p>
</li>
</ul>
<h2 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h2><p>用在某个变量(或特征)值是不是和应变量有显著关系。<br><strong><font color="red">根本思想在于比较理论频数和实际频数的吻合程度或者拟合优度问题。</font></strong><br>$$<br>\chi^2=\sum\frac{(A-T)^2}{T}<br>$$</p>
<p>其中:A是实际值，T为理论值<br>我们需要查询卡方分布的临界值表，将计算的值与临界值比较。<br>查询临界值就需要知道自由度<br>自由度V=（行数-1）*（列数-1）；对四格表，自由度V = 1<br>当P小于等于0.05（置信度95%），认为变量不相关<br>若各理论数与相应实际数相差越小，卡方值越小；如两者相同，则卡方值必为零。</p>
<h2 id="熵总结"><a href="#熵总结" class="headerlink" title="熵总结"></a>熵总结</h2><p><strong>自信息</strong>又称信息量。信息量的度量就等于不确定性的多少。<br>对于已发生的事件$i$，其所提供的信息量为：<br>$$<br>I(p_i)=-\log(p_i)<br>$$</p>
<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>信息熵代表一个分布的信息量,或者编码的平均长度。<br>信息熵用来度量一个事件可能具有多个状态下的信息量，也可以认为是信息量关于事件概率分布的期望值：<br>$$<br>H(X)=-\sum_{i=1}^n p(x_i)\log{p(x_i)}<br>$$<br>随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大。<br>将一维随机变量分布推广到多维随机变量分布，则其 <strong><font color="red">联合熵 (Joint entropy)</font></strong> 为：</p>
<p>$$<br>H(X,Y)=-\sum_{i=1}^n\sum_{j=1}^m{p(x_i,y_j)\log{p(x_i,y_j)}}<br>$$</p>
<h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>条件熵 $H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。条件熵 $H(Y|X)$定义为X给定条件下$Y$的条件概率分布的熵对X的数学期望：<br>$$<br>H(Y|X)=\sum_xp(x)H(Y|X=x)=-\sum_{x,y}p(x,y)\log⁡{p(y|x)}<br>$$<br><strong><font color="red">条件熵$H(Y|X)$相当于联合熵减去单独的熵$H(X)$。</font></strong></p>
<h3 id="相对熵-Relative-entropy"><a href="#相对熵-Relative-entropy" class="headerlink" title="相对熵 (Relative entropy)"></a>相对熵 (Relative entropy)</h3><p>也称 <strong><font color="red">KL散度 (Kullback–Leibler divergence)</font></strong> 。相对熵可以用来衡量两个概率分布之间的差异。<br>设 p(x)、q(x) 是 离散随机变量 X 中取值的两个概率分布，则 p 对q的相对熵是：<br>$$<br>D_{KL}(p||q)=\sum_x p(x) \log{⁡\frac{p(x)}{q(x)}}<br>$$<br>性质：</p>
<ol>
<li>如果 p(x)和 q(x)两个分布相同，那么相对熵等于0</li>
<li>相对熵具有不对称性</li>
<li>≥0  (利用Jensen不等式)</li>
</ol>
<h3 id="交叉熵-Cross-entropy"><a href="#交叉熵-Cross-entropy" class="headerlink" title="交叉熵 (Cross entropy)"></a>交叉熵 (Cross entropy)</h3><p>交叉熵可以来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。<br><strong><font color="red">交叉熵本质上可以看成,用一个猜测的分布的编码方式去编码其真实的分布,得到的平均编码长度或者信息量。</font></strong><br>$$<br>H(p,q)=\sum_x p(x) \log{⁡\frac{1}{q(x)}}=-\sum_x p(x) \log{⁡{q(x)}}<br>$$<br>所以有：<br>$$<br>D_{KL}(p||q)=H(p,q)-H(p)<br>$$<br>当用非真实分布q(x)得到的平均码长比真实分布p(x)得到的平均码长多出的比特数就是相对熵。<br>当H(p)为常量时（在机器学习中，训练数据分布是固定的），最小化相对熵等价于最小化交叉熵；H(p,q)也等价于最大化似然估计.</p>
<blockquote>
<p>交叉熵是指用分布q来表示本来分布p的平均编码长度。可以用来计算学习模型分布与训练分布之间的差异。</p>
</blockquote>
<p><strong>Note</strong>:<br>在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即$DKL(y||\hat{y})$，由于KL散度中的一部分$−H(y)$不变，故在优化过程中，只需要关注交叉熵就可以了。 <strong><font color="red">所以一般在机器学习中直接用用交叉熵做loss，评估模型。</font></strong></p>
<h3 id="互信息（信息增益）"><a href="#互信息（信息增益）" class="headerlink" title="互信息（信息增益）"></a>互信息（信息增益）</h3><p>互信息就是一个联合分布中的两个信息的纠缠程度/或者叫相互影响那部分的信息量.其衡量的是两个随机变量之间的相关性，即一个随机变量中包含的关于另一个随机变量的信息量。<br>$$<br>I(X,Y)=H(X)+H(Y)-H(X,Y) \\<br>I(X,Y)=H(Y)-H(Y|X)<br>$$<br>决策树中的信息增益就是互信息，决策树是采用的上面第二种计算方法，即把分类的不同结果看成不同随机事件Y，然后把当前选择的特征看成X，则信息增益就是当前Y的信息熵减去已知X情况下的信息熵。</p>
<h2 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h2><p>置信区间不能用贝叶斯学派的概率来描述，它属于频率学派的范畴。真值要么在，要么不在。由于在频率学派当中，真值是一个常数，而非随机变量（后者是贝叶斯学派），所以我们不对真值做概率描述。比如，95%置信区间，并不是真值在这个区间内的概率是95%，而应该为100次随机抽样中构造的100个区间如果95次包含了参数真值，那么置信度为95%。</p>
<h2 id="最大似然估计、最大后验估计与贝叶斯估计"><a href="#最大似然估计、最大后验估计与贝叶斯估计" class="headerlink" title="最大似然估计、最大后验估计与贝叶斯估计"></a>最大似然估计、最大后验估计与贝叶斯估计</h2><p>贝叶斯公式可如下表示：<br>$$<br>p(\theta|X) = \frac{p(X|\theta) p(\theta)}{p(X)}<br>$$<br>其中：<br>$p(\theta|X)$ : 后验概率(posterior),通过样本X得到参数$\theta$的概率<br>$p(X|\theta)$ : 似然函数(likehood),通过参数$\theta$得到样本X的概率，通常就是我们的数据集的表现。<br>$p(\theta)$ : 参数$\theta$的先验概率(prior)，一般是根据人的先验知识来得出的。比如人们倾向于认为抛硬币实验会符合先验分布：beta分布。当我们选择beta分布的参数$\alpha=\beta=0.5$时，代表人们认为抛硬币得到正反面的概率都是0.5。</p>
<h3 id="最大似然估计（MLE）"><a href="#最大似然估计（MLE）" class="headerlink" title="最大似然估计（MLE）　　"></a>最大似然估计（MLE）　　</h3><p>　最大似然估计的核心思想是：认为当前发生的事件是概率最大的事件。因此就可以给定的数据集，使得该数据集发生的概率最大来求得模型中的参数。<br>$$<br>p(X|\theta) = \prod p(x_i|\theta)<br>$$<br>最大似然估计只关注当前的样本，也就是只关注当前发生的事情，不考虑事情的先验情况。由于计算简单，而且不需要关注先验知识，因此在机器学习中的应用非常广，最常见的就是逻辑回归。</p>
<h3 id="最大后验估计（MAP）"><a href="#最大后验估计（MAP）" class="headerlink" title="最大后验估计（MAP）"></a>最大后验估计（MAP）</h3><p>和最大似然估计不同的是，<strong><font color="red">最大后验估计中引入了先验概率</font></strong> 。最大似然估计是求参数$θ$, 使似然函数$P(x_i|θ)$最大。最大后验概率估计则是想求$θ$使$P(x_i|θ)P(θ)$最大。求得的$θ$不单单让似然函数大，$θ$自己出现的先验概率也得大。</p>
<p>最大后验估计和最大似然估计的区别：最大后验估计允许我们把先验知识加入到估计模型中，这在样本很少的时候是很有用的（因此朴素贝叶斯在较少的样本下就能有很好的表现），因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。</p>
<h3 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h3><p>最大似然估计和贝叶斯估计都属于参数化估计 。最大似然估计和贝叶斯估计最大区别便在于估计的参数不同，最大似然估计要估计的参数$θ$被当作是固定形式的一个未知变量，然后我们结合真实数据通过最大化似然函数来求解这个固定形式的未知变量！<br>贝叶斯估计则是将参数视为是有某种已知先验分布的随机变量，意思便是这个参数他不是一个固定的未知数，而是符合一定先验分布如：随机变量$θ$符合正态分布等！那么在贝叶斯估计中除了类条件概率密度$p(x|w)$符合一定的先验分布，参数$θ$也符合一定的先验分布。我们通过贝叶斯规则将参数的先验分布转化成后验分布进行求解。<br>贝叶斯估计和极大后验估计有点相似，都是以最大化后验概率为目的。区别在于：极大后验估计在计算后验概率的时候，把分母$p(X)$给忽略了，在进行贝叶斯估计的时候则不能忽略,并且贝叶斯估计要<strong>计算整个后验概率的概率分布</strong>。<br>贝叶斯估计的应用有LDA主题模型。LDA主题模型通过共轭分布的特性来求出主题分布和词分布。</p>
</div><div class="tags"><a href="/tags/machine-learning/">machine learning</a></div><div class="post-nav"><a class="pre" href="/2019/04/24/集成学习/">集成学习</a><a class="next" href="/2019/04/23/决策树模型/">决策树模型</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/algorithms/" style="font-size: 15px;">algorithms</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/Algorithm-practice/" style="font-size: 15px;">Algorithm practice</a> <a href="/tags/Books-Note/" style="font-size: 15px;">Books Note</a> <a href="/tags/Recommendation-System/" style="font-size: 15px;">Recommendation System</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/06/20/RNN基础/">RNN基础</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/28/深度排序模型概述（一）Wide&Deep_xDeepFM/">深度排序模型概述（一）Wide&Deep/xDeepFM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/FM系列/">FM系列</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/推荐系统概述（二）/">推荐系统概述（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/19/推荐系统概述（一）/">推荐系统概述（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/概率图模型/">概率图模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/06/百面机器学习/">百面机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/聚类算法一览/">聚类算法一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/EM算法/">EM算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/降维算法一览/">降维算法一览</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="https://github.com/helloJamest" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Jamest.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>