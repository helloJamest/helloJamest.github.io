<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>逻辑回归模型 | Jamest</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">逻辑回归模型</h1><a id="logo" href="/.">Jamest</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">逻辑回归模型</h1><div class="post-meta">Apr 21, 2019</div><div class="post-content"><p>逻辑回归模型是针对线性可分问题的一种易于实现而且性能优异的分类模型。<br><strong>它假设数据服从<font color="red">伯努利分布</font>,通过<font color="red">极大化似然函数</font>的方法，运用<font color="red">梯度下降法</font>来求解参数，来达到将数据二分类的目的。</strong></p>
<a id="more"></a>
<h2 id="算法推导"><a href="#算法推导" class="headerlink" title="算法推导"></a>算法推导</h2><ul>
<li>引入几率比（odds）：指一个事件发生的概率与不发生概率的比值。对其求log，可得：<br>$$<br>logit(p) = \log{\frac{p}{1-p}}<br>$$</li>
</ul>
<p>将对数几率记为输入特征值的线性表达式,可得<br>$$<br>logit(P(Y=1|X)) = w^Tx<br>$$<br>对于某一样本属于特定类别的概率，为$logit$函数的反函数，称为$logistic$函数，即$sigmoid$函数:<br>$$<br>\phi(x) = \frac{1}{1+e^{-z}}<br>$$</p>
<blockquote>
<p>逻辑斯蒂回归采用sigmoid函数作为激励函数</p>
</blockquote>
<ul>
<li><p>逻辑斯蒂回归模型定义：<br>$$<br>P(Y=1|X) = \frac{e^{wx}}{1+e^{wx}}<br>$$<br>$$<br>P(Y=0|X) = \frac{1}{1+e^{wx}}<br>$$<br>可知，输出$Y=1$的对数几率是输入$x$的线性函数。</p>
</li>
<li><p>对于给定的训练数据集$T$,可以应用<font color="red">极大似然估计法</font>估计模型参数，假设模型概率分布是：<br>$$<br>P(Y=1|X) = \pi{(x)}<br>$$<br>$$<br>P(Y=0|X) = 1-\pi{(x)}<br>$$<br>似然函数为：<br>$$<br>\prod_{i=1}^N{[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}}<br>$$<br>对数似然函数为：<br>$$<br>L(w)=\sum_{i=1}^N{[y_i\log{\pi(x_i)}+(1-y_i)\log{(1-\pi(x_i))}]}<br>$$</p>
</li>
</ul>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ul>
<li><p>逻辑斯蒂回归模型的优点有：</p>
<ul>
<li>形式简单，模型的<strong>可解释性</strong>非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</li>
<li>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</li>
<li>训练<strong>速度较快</strong>。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化$sgd$发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</li>
<li>资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。</li>
<li><strong>方便输出结果调整</strong>。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</li>
</ul>
</li>
<li><p>逻辑斯蒂回归模型的缺点有：</p>
<ul>
<li><strong>准确率并不是很高</strong>。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</li>
<li>很难处理<strong>数据不平衡</strong>的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</li>
<li>处理<strong>非线性数据</strong>较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题</li>
<li>逻辑回归本身<strong>无法筛选特征</strong>。有时候，我们会用GBDT来筛选特征，然后再上逻辑回归。</li>
</ul>
</li>
</ul>
<h2 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h2><blockquote>
<p>LR的损失函数为什么要使用极大似然函数作为损失函数？</p>
</blockquote>
<p>将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。<br>梯度更新速度只和$x_{ij}$，$y_{i}$相关。和$sigmod$函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。<br>为什么不选平方损失函数呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和$sigmod$函数本身的梯度是很相关的。$sigmod$函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p>
<blockquote>
<p>LR的损失函数为什么要使用$sigmoid$函数，背后的数学原理是什么？</p>
</blockquote>
<p>LR假设数据服从伯努利分布,所以我们只需要知道 $P(Y|X)$；其次我们需要一个线性模型，所以 $P(Y|X) = f(wx)$。接下来我们就只需要知道 $f$ 是什么就行了。而我们可以通过最大熵原则推出的这个$f$，就是$sigmoid$。<br><strong>$sigmoid$是在伯努利分布和广义线性模型的假设推导出来的。</strong></p>
<blockquote>
<p>为什么LR的输入特征一般是离散的而不是连续的？</p>
</blockquote>
<p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ol>
<li>离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li>
<li>离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li>
</ol>
</div><div class="tags"><a href="/tags/algorithms/">algorithms</a></div><div class="post-nav"><a class="pre" href="/2019/04/22/支持向量机模型/">支持向量机模型</a><a class="next" href="/2019/04/19/最大熵模型/">最大熵模型</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/algorithms/" style="font-size: 15px;">algorithms</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/Algorithm-practice/" style="font-size: 15px;">Algorithm practice</a> <a href="/tags/Books-Note/" style="font-size: 15px;">Books Note</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/概率图模型/">概率图模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/06/百面机器学习/">百面机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/聚类算法一览/">聚类算法一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/EM算法/">EM算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/降维算法一览/">降维算法一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/29/xgboost-lightgbm调参指南/">xgboost&lightgbm调参指南</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/24/集成学习/">集成学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/机器学习基础问题/">机器学习基础概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/决策树模型/">决策树模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/22/支持向量机模型/">支持向量机模型</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="https://github.com/helloJamest" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Jamest.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>