<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>xgboost&amp;lightgbm调参指南 | Jamest</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">xgboost&amp;lightgbm调参指南</h1><a id="logo" href="/.">Jamest</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Accueil</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> À propos</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">xgboost&amp;lightgbm调参指南</h1><div class="post-meta">Apr 29, 2019</div><div class="post-content"><p>本文重点阐述了xgboost和lightgbm的主要参数和调参技巧，其理论部分可见<a href="https://hellojamest.github.io/2019/04/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">集成学习</a>,以下内容主要来自<a href="https://xgboost.readthedocs.io/en/latest/parameter.html" target="_blank" rel="noopener">xgboost</a>和<a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" target="_blank" rel="noopener">LightGBM</a>的官方文档。</p>
<h1 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h1><p>Xgboost参数主要分为三大类：<br>General Parameters（通用参数）：设置整体功能<br>Booster Parameters（提升参数）：选择你每一步的booster(树or回归）<br>Learning Task Parameters（学习任务参数）：指导优化任务的执行</p>
<h2 id="General-Parameters（通用参数）"><a href="#General-Parameters（通用参数）" class="headerlink" title="General Parameters（通用参数）"></a>General Parameters（通用参数）</h2><ul>
<li>booster [default=gbtree]<br>选择每次迭代过程中需要运行的模型，一共有两种选择：gbtree和gblinear。gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算。缺省值为gbtree</li>
<li>silent [default=0]<br>取0时表示打印出运行时信息，取1时表示以缄默方式运行，不打印运行时信息。缺省值为0</li>
<li>nthread [default to maximum number of threads available if not set]<br>XGBoost运行时的线程数。缺省值是当前系统可以获得的最大线程数</li>
</ul>
<p>剩余两个参数是Xgboost自动指定的，无需设置。</p>
<h2 id="Booster-Parameters（提升参数）"><a href="#Booster-Parameters（提升参数）" class="headerlink" title="Booster Parameters（提升参数）"></a>Booster Parameters（提升参数）</h2><p><strong>虽然有两种类型的booster，但是我们这里只介绍tree。因为tree的性能比线性回归好得多，因此我们很少用线性回归。</strong></p>
<ul>
<li>eta [default=0.3]<br>学习率，可以缩减每一步的权重值，使得模型更加健壮：<br>典型值一般设置为：0.01-0.2<br>取值范围为：[0,1]</li>
<li>gamma [default=0]<br>这个指定了一个结点被分割时，所需要的最小损失函数减小的大小。这个值一般来说需要根据损失函数来调整。<br>range: [0,∞]</li>
<li>max_depth [default=6]<br>数的最大深度。缺省值为6<br>这个可以用来控制过拟合，典型值是3-10。<br>取值范围为：[1,∞]</li>
<li>min_child_weight [default=1]<br>孩子节点中最小的样本权重和。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。该值越大算法越保守<br>取值范围为: [0,∞]</li>
<li>subsample [default=1]<br>用于训练模型的子样本占整个样本集合的比例。如果设置为0.5则意味着XGBoost将随机的冲整个样本集合中随机的抽取出50%的子样本建立树模型，这能够防止过拟合。<br>取值范围为：(0,1]</li>
<li>colsample_bytree [default=1]<br>在建立树时对特征采样的比例。缺省值为1<br>取值范围：(0,1]</li>
<li>lambda [default=1, alias: reg_lambda]<br>L2正则化。</li>
<li>alpha [default=0, alias: reg_alpha]<br>L1正则化，主要用在数据维度很高的情况下，可以提高运行速度。</li>
<li>scale_pos_weight, [default=1]<br>在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛。</li>
</ul>
<h2 id="Learning-Task-Parameters（学习任务参数）"><a href="#Learning-Task-Parameters（学习任务参数）" class="headerlink" title="Learning Task Parameters（学习任务参数）"></a>Learning Task Parameters（学习任务参数）</h2><ul>
<li><p>objective [ default=reg:linear ]<br>定义学习任务及相应的学习目标，可选的目标函数如下：<br>“reg:linear” –线性回归。<br>“reg:logistic” –逻辑回归。<br>“binary:logistic” –二分类的逻辑回归问题，输出为概率。<br>“binary:logitraw” –二分类的逻辑回归问题，输出的结果为wTx。<br>“count:poisson” –计数问题的poisson回归，输出结果为poisson分布。<br>“multi:softmax” –让XGBoost采用softmax目标函数处理多分类问题，同时需要设置参数num_class（类别个数）<br>“multi:softprob” –和softmax一样，但是输出的是ndata * nclass的向量，可以将该向量reshape成ndata行nclass列的矩阵。每行数据表示样本所属于每个类别的概率。</p>
</li>
<li><p>eval_metric [ default according to objective ]<br>评估方法，主要用来验证数据，根据一个学习目标会默认分配一个评估指标<br>“rmse”:均方根误差（回归任务）<br>“logloss”：<br>“error”<br>“mlogloss”<br>“merror”<br>“auc”</p>
</li>
<li>seed [ default=0 ]<br>随机数的种子。缺省值为0</li>
</ul>
<h2 id="调参方法"><a href="#调参方法" class="headerlink" title="调参方法"></a>调参方法</h2><p>调参的通用方法：<br>选择一个相对较高的学习率。通常来说学习率设置为0.1。但是对于不同的问题可以讲学习率设置在0.05-0.3。通过交叉验证来寻找符合学习率的最佳树的个数。<br>当确定好学习率与最佳树的个数时，调整树的某些特定参数。比如：max_depth, min_child_weight, gamma, subsample, colsample_bytree<br>调整正则化参数 ，比如： lambda, alpha。这个主要是为了减少模型复杂度和提高运行速度的。适当地减少过拟合。<br>降低学习速率，选择最优参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line">params=&#123;</span><br><span class="line"><span class="string">'booster'</span>:<span class="string">'gbtree'</span>,</span><br><span class="line"><span class="string">'objective'</span>: <span class="string">'multi:softmax'</span>, <span class="comment">#多分类的问题</span></span><br><span class="line"><span class="string">'num_class'</span>:<span class="number">10</span>, <span class="comment"># 类别数，与 multisoftmax 并用</span></span><br><span class="line"><span class="string">'gamma'</span>:<span class="number">0.1</span>,  <span class="comment"># 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line"><span class="string">'max_depth'</span>:<span class="number">12</span>, <span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line"><span class="string">'lambda'</span>:<span class="number">2</span>,  <span class="comment"># 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。</span></span><br><span class="line"><span class="string">'subsample'</span>:<span class="number">0.7</span>, <span class="comment"># 随机采样训练样本</span></span><br><span class="line"><span class="string">'colsample_bytree'</span>:<span class="number">0.7</span>, <span class="comment"># 生成树时进行的列采样</span></span><br><span class="line"><span class="string">'min_child_weight'</span>:<span class="number">3</span>,</span><br><span class="line"><span class="comment"># 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言</span></span><br><span class="line"><span class="comment">#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。</span></span><br><span class="line"><span class="comment">#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></span><br><span class="line"><span class="string">'silent'</span>:<span class="number">0</span> ,<span class="comment">#设置成1则没有运行信息输出，最好是设置为0.</span></span><br><span class="line"><span class="string">'eta'</span>: <span class="number">0.007</span>, <span class="comment"># 如同学习率</span></span><br><span class="line"><span class="string">'seed'</span>:<span class="number">1000</span>,</span><br><span class="line"><span class="string">'nthread'</span>:<span class="number">7</span>,<span class="comment"># cpu 线程数</span></span><br><span class="line"><span class="comment">#'eval_metric': 'auc'</span></span><br><span class="line">&#125;</span><br><span class="line">model = xgb.train(plst, xgb_train, num_rounds, watchlist,early_stopping_rounds=<span class="number">100</span>)</span><br><span class="line">model.save_model(<span class="string">'./model/xgb.model'</span>) <span class="comment"># 用于存储训练出的模型</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"best best_ntree_limit"</span>,model.best_ntree_limit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Grid seach on subsample and max_features</span></span><br><span class="line"></span><br><span class="line">param_test1 = &#123;</span><br><span class="line">    <span class="string">'max_depth'</span>:list(range(<span class="number">3</span>,<span class="number">10</span>,<span class="number">2</span>)),</span><br><span class="line">    <span class="string">'min_child_weight'</span>:list(range(<span class="number">1</span>,<span class="number">6</span>,<span class="number">2</span>))</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">参数说明;</span></span><br><span class="line"><span class="string">n_estimators：基学习器的个数(第一步得到)</span></span><br><span class="line"><span class="string">scoring：得分排名依据，因为是越高越好，这是mean_squared_error取负</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">gsearch1 = GridSearchCV(estimator = XGBRegressor( learning_rate =<span class="number">0.1</span>, n_estimators=<span class="number">33</span>, max_depth=<span class="number">5</span>,</span><br><span class="line">                                        min_child_weight=<span class="number">1</span>, gamma=<span class="number">0</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.8</span>,</span><br><span class="line">                                        objective= <span class="string">'gpu:reg:linear'</span>, nthread=<span class="number">4</span>, scale_pos_weight=<span class="number">1</span>, seed=<span class="number">27</span>),</span><br><span class="line">                       param_grid = param_test1, scoring=<span class="string">'neg_mean_squared_error'</span>,n_jobs=<span class="number">1</span>,iid=<span class="keyword">False</span>, cv=<span class="number">5</span>)</span><br><span class="line">gsearch1.fit(X,y)</span><br><span class="line">print(gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#进一步探索参数max_depth，min_child_weight(前面得到最好的是&#123;‘max_depth’: 5, ‘min_child_weight’: 1&#125;)</span></span><br><span class="line"><span class="comment">#Grid seach on subsample and max_features</span></span><br><span class="line"></span><br><span class="line">param_test2 = &#123;</span><br><span class="line">    <span class="string">'max_depth'</span>:[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">    <span class="string">'min_child_weight'</span>:[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]  <span class="comment">#范围: [0,∞]</span></span><br><span class="line">&#125;</span><br><span class="line">gsearch2 = GridSearchCV(estimator = XGBRegressor( learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">33</span>, max_depth=<span class="number">5</span>,</span><br><span class="line">                                        min_child_weight=<span class="number">2</span>, gamma=<span class="number">0</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.8</span>,</span><br><span class="line">                                        objective= <span class="string">'gpu:reg:linear'</span>, nthread=<span class="number">4</span>, scale_pos_weight=<span class="number">1</span>,seed=<span class="number">27</span>),</span><br><span class="line">                       param_grid = param_test2, scoring=<span class="string">'neg_mean_squared_error'</span>,n_jobs=<span class="number">1</span>,iid=<span class="keyword">False</span>, cv=<span class="number">5</span>)</span><br><span class="line">gsearch2.fit(X,y)</span><br><span class="line">print(gsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_)</span><br><span class="line"><span class="comment">#继续搜索参数min_child_weight &#123;‘max_depth’: 5, ‘min_child_weight’: 2&#125;</span></span><br><span class="line"><span class="comment">#Grid seach on subsample and max_features</span></span><br><span class="line"><span class="comment">#-3775.018211540544</span></span><br><span class="line">param_test2b = &#123;</span><br><span class="line">    <span class="string">'min_child_weight'</span>:[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>]</span><br><span class="line">&#125;</span><br><span class="line">gsearch2b = GridSearchCV(estimator = XGBRegressor( learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">33</span>, max_depth=<span class="number">5</span>,</span><br><span class="line">                                        min_child_weight=<span class="number">2</span>, gamma=<span class="number">0</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.8</span>,</span><br><span class="line">                                        objective= <span class="string">'gpu:reg:linear'</span>, nthread=<span class="number">4</span>, scale_pos_weight=<span class="number">1</span>,seed=<span class="number">27</span>),</span><br><span class="line">                       param_grid = param_test2b, scoring=<span class="string">'neg_mean_squared_error'</span>,n_jobs=<span class="number">1</span>,iid=<span class="keyword">False</span>, cv=<span class="number">5</span>)</span><br><span class="line">gsearch2b.fit(X,y)</span><br><span class="line">print(gsearch2b.cv_results_, gsearch2b.best_params_, gsearch2b.best_score_)</span><br><span class="line"><span class="comment">#调整参数gamma，range: [0,∞]，default=0</span></span><br><span class="line"><span class="comment">#Grid seach on subsample and max_features</span></span><br><span class="line"><span class="comment">#在树的叶子节点上进行进一步划分所需的最小损失。越大，算法就越保守。</span></span><br><span class="line">param_test3 = &#123;</span><br><span class="line">    <span class="string">'gamma'</span>:[i/<span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">5</span>)]</span><br><span class="line">&#125;</span><br><span class="line">gsearch3 = GridSearchCV(estimator = XGBRegressor( learning_rate =<span class="number">0.1</span>, n_estimators=<span class="number">33</span>, max_depth=<span class="number">5</span>,</span><br><span class="line">                                        min_child_weight=<span class="number">2</span>, gamma=<span class="number">0</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.8</span>,</span><br><span class="line">                                        objective= <span class="string">'gpu:reg:linear'</span>, nthread=<span class="number">4</span>, scale_pos_weight=<span class="number">1</span>,seed=<span class="number">27</span>),</span><br><span class="line">                       param_grid = param_test3, scoring=<span class="string">'neg_mean_squared_error'</span>,n_jobs=<span class="number">1</span>,iid=<span class="keyword">False</span>, cv=<span class="number">5</span>)</span><br><span class="line">gsearch3.fit(X,y)</span><br><span class="line">print(gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_)</span><br><span class="line"><span class="comment">#调整参数subsample，colsample_bytree</span></span><br><span class="line"><span class="comment">#Grid seach on subsample and max_features</span></span><br><span class="line">param_test4 = &#123;</span><br><span class="line">    <span class="string">'subsample'</span>:[i/<span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>,<span class="number">10</span>)],</span><br><span class="line">    <span class="string">'colsample_bytree'</span>:[i/<span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>,<span class="number">10</span>)]</span><br><span class="line">&#125;</span><br><span class="line">gsearch4 = GridSearchCV(estimator = XGBRegressor( learning_rate =<span class="number">0.1</span>, n_estimators=<span class="number">53</span>, max_depth=<span class="number">5</span>,</span><br><span class="line">                                        min_child_weight=<span class="number">2</span>, gamma=<span class="number">0</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.8</span>,</span><br><span class="line">                                        objective= <span class="string">'gpu:reg:linear'</span>, nthread=<span class="number">4</span>, scale_pos_weight=<span class="number">1</span>,seed=<span class="number">27</span>),</span><br><span class="line">                       param_grid = param_test4, scoring=<span class="string">'neg_mean_squared_error'</span>,n_jobs=<span class="number">1</span>,iid=<span class="keyword">False</span>, cv=<span class="number">5</span>)</span><br><span class="line">a=gsearch4.fit(train[predictors],train[target])</span><br><span class="line">print(a)</span><br><span class="line">gsearch4.fit(X,y)</span><br><span class="line">print(gsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h1><p>LightGBM中的主要调节的参数包括核心参数、学习控制参数、IO 参数、目标参数、度量参数等。</p>
<h2 id="Core-Parameters（核心参数）"><a href="#Core-Parameters（核心参数）" class="headerlink" title="Core Parameters（核心参数）"></a>Core Parameters（核心参数）</h2><ul>
<li>task [default=train]<br>数据的用途    选择 train，predict或者convert_model（将模型文件转换成 if-else 格式）：</li>
<li>objective [default=regression]<br>模型的用途<br>‘regression’表示回归任务，但是使用L2损失函数。<br>‘regression_l1’：表示回归任务，但是使用L1损失函数。<br>‘huber’： 表示回归任务，但是使用huber 损失函数。<br>‘fair’： 表示回归任务，但是使用fair 损失函数。<br>‘binary’： 表示二分类任务，使用对数损失函数作为目标函数。<br>‘multiclass’： 表示多分类任务，使用softmax 函数作为目标函数。必须设置num_class 参数<br>‘multiclassova’ ： 表示多分类任务，使用one-vs-all 的二分类目标函数。必须设置num_class 参数<br>‘xentropy’： 目标函数为交叉熵（同时具有可选择的线性权重）。要求标签是[0,1] 之间的数值。<br>‘xentlambda’ ： 替代了参数化的cross_entropy 。要求标签是[0,1]之间的数值。</li>
<li>boosting [default=gbdt]<br>给出基学习器模型算法。可以为：<br>‘gbdt’： 表示传统的梯度提升决策树。默认值为’gbdt’<br>‘rf’： 表示随机森林。<br>‘dart’： 表示带dropout 的gbdt<br>goss：表示Gradient-based One-Side Sampling 的gbdt (基于梯度的单侧采样)</li>
<li>data, [default=””]<br>训练数据, LightGBM 将会使用这个数据进行训练</li>
<li>valid, [default=””]<br>验证/测试 数据, LightGBM 将输出这些数据的度量</li>
<li>num_iterations, [default=100]<br>boosting的迭代次数。<ul>
<li>对于python/R包，该参数是被忽略的。对于python，使用train()/cv()的输入参数num_boost_round来代替。</li>
</ul>
</li>
<li>learning_rate, [default=0.1]<br>学习率</li>
<li>num_leaves, [default=31]<br>一棵树上的叶子数。</li>
<li>tree_learner,[default=serial]<br>主要用于并行学习。 默认为’serial’单台机器。</li>
<li>num_threads, [default=OpenMP_default]<br>LightGBM 的线程数</li>
<li>device, [default=cpu]<br>指定计算设备。默认为’cpu’。 可以为’gpu’,’cpu’。<ul>
<li>为了加快学习速度，GPU 默认使用32位浮点数来求和。你可以设置gpu_use_dp=True 来启动64位浮点数，但是它会使得训练速度降低。</li>
</ul>
</li>
</ul>
<h2 id="Learning-Control-Parameters（学习控制参数）"><a href="#Learning-Control-Parameters（学习控制参数）" class="headerlink" title="Learning Control Parameters（学习控制参数）"></a>Learning Control Parameters（学习控制参数）</h2><ul>
<li>max_depth, [default=-1]<br>限制树模型的最大深度。如果小于0，则表示没有限制。</li>
<li>min_data_in_leaf, [default=20]<br>一个叶子上数据的最小数量. 可以用来处理过拟合.</li>
<li>min_sum_hessian_in_leaf, [default=1e-3]<br>一个叶子上的最小 hessian 和也就是叶节点样本权重之和的最小值）， 类似于 min_data_in_leaf, 可以用来处理过拟合.</li>
<li>feature_fraction, [default=1.0]<br>如果小于1.0，则lightgbm 会在每次迭代中随机选择部分特征。如0.8 表示：在每棵树训练之前选择80% 的特征来训练。</li>
<li>feature_fraction_seed, [default=2]<br>feature_fraction 的随机数种子</li>
<li>bagging_fraction, [default=1]<br>类似于 feature_fraction, 但是它将在不进行重采样的情况下随机选择部分数据<ul>
<li>Note: 为了启用 bagging, bagging_freq 应该设置为非零值</li>
</ul>
</li>
<li>bagging_freq, [default=0]<br>bagging 的频率, 0 意味着禁用 bagging. k 意味着每 k 次迭代执行bagging</li>
<li>bagging_seed , [default=3]<br>bagging 随机数种子</li>
<li>early_stopping_round, [default=0]<br>如果一个验证集的度量在 early_stopping_round 循环中没有提升, 将停止训练</li>
<li>lambda_l1, [default=0]<br>L1 正则</li>
<li>lambda_l2, [default=0]<br>L2 正则</li>
<li>min_split_gain, [default=0]<br>执行切分的最小增益</li>
<li>lambda_l1, [default=0]<br>L1 正则</li>
<li>feature_fraction_seed, [default=2]<br>feature_fraction 的随机数种子</li>
<li>feature_fraction_seed, [default=2]<br>feature_fraction 的随机数种子</li>
<li>feature_fraction_seed, [default=2]<br>feature_fraction 的随机数种子</li>
</ul>
<h2 id="IO-Parameters（IO-参数）"><a href="#IO-Parameters（IO-参数）" class="headerlink" title="IO Parameters（IO 参数）"></a>IO Parameters（IO 参数）</h2><ul>
<li>max_bin, [default=255]<br>表示最大的桶的数量。lightgbm 会根据它来自动压缩内存。如max_bin=255 时，则lightgbm 将使用uint8 来表示特征的每一个值。</li>
<li>min_data_in_bin, [default=3]<br>表示每个桶的最小样本数。该方法可以避免出现一个桶只有一个样本的情况。</li>
<li>data_random_seed, [default=1]<br>表示并行学习数据分隔中的随机数种子。默认为1它不包括特征并行。</li>
<li>output_model, [default=LightGBM_model.txt]<br>表示训练中输出的模型被保存的文件的文件名。</li>
<li>input_model, [default=””]<br>表示输入模型的文件的文件名。默认空字符串。对于prediction任务，该模型将用于预测数据，对于train任务，训练将从该模型继续</li>
<li>output_result, [default=LightGBM_predict_result.txt]<br>prediction 任务的预测结果文件名</li>
<li>pre_partition, [default=false]<br>用于并行学习(不包括功能并行). 如果为true，则不同的机器使用不同的partition 来训练。</li>
<li>is_sparse, [default=true]<br>用于 enable/disable 稀疏优化. 设置 false 就禁用稀疏优化</li>
<li>two_round, [default=false]<br>一个布尔值，指示是否启动两次加载。默认值为False，表示只需要进行一次加载。默认情况下，lightgbm 会将数据文件映射到内存，然后从内存加载特征，这将提供更快的数据加载速度。但是当数据文件很大时，内存可能会被耗尽。如果数据文件太大，则将它设置为True</li>
<li>save_binary, [default=1]<br>表示是否将数据集（包括验证集）保存到二进制文件中。默认值为False。如果为True，则可以加快数据的加载速度。</li>
<li><p>verbosity, [default=1]<br>表示是否输出中间信息。默认值为1。如果小于0，则仅仅输出critical 信息；如果等于0，则还会输出error,warning 信息； 如果大于0，则还会输出info 信息。</p>
</li>
<li><p>header, [default=false]<br>如果输入数据有标识头, 则在此处设置 true</p>
</li>
<li>label , [default=””]<br>表示标签列。默认为空字符串。你也可以指定一个整数，如label=0 表示第0列是标签列。你也可以为列名添加前缀，如label=prefix:label_name</li>
<li>weight , [default=””]<br>一个字符串，表示样本权重列。默认为空字符串。你也可以指定一个整数，如weight=0 表示第0列是权重列。注意：它是剔除了标签列之后的索引。假如标签列为0，权重列为1，则这里weight=0。你也可以为列名添加前缀，如weight=prefix:weight_name</li>
<li>query, [default=””]<br>表示query/group ID 列。</li>
<li>ignore_column , [default=””]<br>表示训练中忽略的一些列，默认为空字符串。可以用数字做索引，如ignore_column=0,1,2 表示第0,1,2 列将被忽略。注意：它是剔除了标签列之后的索引。</li>
<li>categorical_feature, [default=””]<br>指定category 特征的列。默认为空字符串。可以用数字做索引，如categorical_feature=0,1,2 表示第0,1,2 列将作为category 特征。注意：它是剔除了标签列之后的索引。你也可以为列名添加前缀，如categorical_feature=prefix:cat_name1,cat_name2 在categorycal 特征中，负的取值被视作缺失值。</li>
<li>predict_leaf_index[default=false]<br>表示是否预测每个样本在每棵树上的叶节点编号。默认为False。在预测时，每个样本都会被分配到每棵树的某个叶子节点上。该参数就是要输出这些叶子节点的编号。该参数只用于prediction 任务。</li>
<li>bin_construct_sample_cnt [default=200000]<br>表示用来构建直方图的样本的数量。默认为200000。如果数据非常稀疏，则可以设置为一个更大的值，如果设置更大的值，则会提供更好的训练效果，但是会增加数据加载时间。</li>
<li>num_iteration_predict[default=-1]<br>表示在预测中使用多少棵子树。默认为-1。小于等于0表示使用模型的所有子树。该参数只用于prediction 任务。</li>
</ul>
<h2 id="Objective-Parameters（目标参数）"><a href="#Objective-Parameters（目标参数）" class="headerlink" title="Objective Parameters（目标参数）"></a>Objective Parameters（目标参数）</h2><ul>
<li>sigmoid[default=1]<br>sigmoid 函数的参数. 将用于 binary 分类 和 lambdarank。</li>
<li>scale_pos_weight[default=1]<br>用于调整正样本的权重，默认值为0，用于二分类任务。</li>
<li>is_unbalance[default=false]<br>指示训练数据是否均衡的。默认为True。用于二分类任务。</li>
<li>num_class[default=1]<br>指示了多分类任务中的类别数量。默认为1，用于多分类任务。</li>
<li>reg_sqrt[default=false]<br>一个布尔值，默认为False。如果为True，则拟合的结果为：$\sqrt{label}$。同时预测的结果被自动转换为：${pred}^2$。它用于回归任务。</li>
</ul>
<h2 id="Metric-Parameters（度量参数）"><a href="#Metric-Parameters（度量参数）" class="headerlink" title="Metric Parameters（度量参数）"></a>Metric Parameters（度量参数）</h2><ul>
<li>metric[default={l2 for regression}, {binary_logloss for binary classification}, {ndcg for lambdarank}]<br>度量的指标，默认为：对于回归问题，使用l2 ；对于二分类问题，使用binary_logloss；对于lambdarank 问题，使用ndcg。如果有多个度量指标，则用逗号, 分隔。<ul>
<li>‘l1’ 或者 mean_absolute_error或者 mae或者 regression_l1： 表示绝对值损失</li>
<li>‘l2’ 或者mean_squared_error或者 mse或者 regression_l2或者 regression：表示平方损失</li>
<li>‘l2_root’ 或者root_mean_squared_error或者 rmse：表示开方损失</li>
<li>‘quantile’： 表示Quantile 回归中的损失</li>
<li>‘mape’ 或者 ‘mean_absolute_percentage_error’ ：表示MAPE 损失</li>
<li>‘map’ 或者’mean_average_precision’： 表示平均的精度</li>
<li>‘auc’： 表示AUC</li>
<li>‘binary_logloss’或者’binary’： 表示二类分类中的对数损失函数</li>
<li>‘binary_error’： 表示二类分类中的分类错误率</li>
<li>‘multi_logloss’或者 ‘multiclass’或者 ‘softmax’或者 ‘multiclassova’或者 ‘multiclass_ova’,或者’ova’或者 ‘ovr’： 表示多类分类中的对数损失函数</li>
<li>‘multi_error’： 表示多分类中的分类错误率</li>
<li>‘xentropy’或者’cross_entropy’： 表示交叉熵</li>
<li>‘xentlambda’ 或者’cross_entropy_lambda’： 表示intensity 加权的交叉熵</li>
<li>‘kldiv’或者’kullback_leibler’： 表示KL 散度</li>
</ul>
</li>
<li>metric_freq[default=1]<br>每隔多少次输出一次度量结果。默认为1。</li>
<li>train_metric [default=false]<br>如果为True，则在训练时就输出度量结果。</li>
</ul>
<h2 id="调参方法-1"><a href="#调参方法-1" class="headerlink" title="调参方法"></a>调参方法</h2><p><strong>针对 leaf-wise 树的参数优化</strong>：</p>
<ul>
<li>num_leaves：控制了叶节点的数目。它<font color="red">是控制树模型复杂度的主要参数。</font><blockquote>
<p>如果是level-wise，则该参数为$2^{depth}$，其中$depth$为树的深度。但是当叶子数量相同时，leaf-wise的树要远远深过level-wise树，非常容易导致过拟合。因此应该让num_leaves小于$2^{depth}$。在leaf-wise树中，并不存在$depth$的概念。因为不存在一个从$leaves$到$depth$的合理映射。</p>
</blockquote>
</li>
<li>min_data_in_leaf：每个叶节点的最少样本数量。它是<font color="red">处理leaf-wise树的过拟合的重要参数。</font>将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合。</li>
<li>max_depth： 控制了树的最大深度。该参数可以显式的限制树的深度。</li>
</ul>
<p><strong>针对更快的训练速度</strong>：</p>
<ul>
<li>通过设置 bagging_fraction 和 bagging_freq 参数来使用 bagging 方法</li>
<li>通过设置 feature_fraction 参数来使用特征的子抽样</li>
<li>使用较小的 max_bin</li>
<li>使用 save_binary 在未来的学习过程对数据加载进行加速</li>
</ul>
<p><strong>获取更好的准确率</strong>：</p>
<ul>
<li>使用较大的 max_bin （学习速度可能变慢）</li>
<li>使用较小的 learning_rate 和较大的 num_iterations</li>
<li>使用较大的 num_leaves （可能导致过拟合）</li>
<li>使用更大的训练数据</li>
<li>尝试 dart</li>
</ul>
<p><strong>缓解过拟合</strong>：</p>
<ul>
<li>使用较小的 max_bin</li>
<li>使用较小的 num_leaves</li>
<li>使用 min_data_in_leaf 和 min_sum_hessian_in_leaf</li>
<li>通过设置 bagging_fraction 和 bagging_freq 来使用 bagging</li>
<li>通过设置 feature_fraction 来使用特征子抽样</li>
<li>使用更大的训练数据</li>
<li>使用 lambda_l1, lambda_l2 和 min_gain_to_split 来使用正则</li>
<li>尝试 max_depth 来避免生成过深的树</li>
</ul>
</div><div class="tags"><a href="/tags/Algorithm-practice/">Algorithm practice</a></div><div class="post-nav"><a class="pre" href="/2019/05/03/降维算法一览/">降维算法一览</a><a class="next" href="/2019/04/24/集成学习/">集成学习</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Catégories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/algorithms/" style="font-size: 15px;">algorithms</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/Algorithm-practice/" style="font-size: 15px;">Algorithm practice</a> <a href="/tags/Books-Note/" style="font-size: 15px;">Books Note</a> <a href="/tags/Recommendation-System/" style="font-size: 15px;">Recommendation System</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Récent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/推荐系统概述（二）/">推荐系统概述（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/19/推荐系统概述（一）/">推荐系统概述（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/概率图模型/">概率图模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/06/百面机器学习/">百面机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/聚类算法一览/">聚类算法一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/EM算法/">EM算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/降维算法一览/">降维算法一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/29/xgboost-lightgbm调参指南/">xgboost&lightgbm调参指南</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/24/集成学习/">集成学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/机器学习基础问题/">机器学习基础概念</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> À suivre</i></div><ul></ul><a href="https://github.com/helloJamest" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Jamest.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>