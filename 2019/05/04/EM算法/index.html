<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>EM算法 | Jamest</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">EM算法</h1><a id="logo" href="/.">Jamest</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">EM算法</h1><div class="post-meta">May 4, 2019</div><div class="post-content"><p>EM算法，即最大期望算法（Expectation-maximization algorithm），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型 <strong><font color="red">依赖于无法观测的隐性变量</font></strong>。<br><a id="more"></a><br>最大期望算法经过两个步骤交替进行计算，<br>第一步是<strong>计算期望（E）</strong>，利用对隐藏变量的现有估计值，计算其最大似然估计值；<br>第二步是<strong>最大化（M）</strong>，最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。<br>多数情况下我们是根据已知条件来推算结果，而最大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。似然描述的是结果已知的情况下，该事件在不同条件下发生的可能性，似然函数的值越大说明该事件在对应的条件下发生的可能性越大。</p>
<h1 id="算法推导"><a href="#算法推导" class="headerlink" title="算法推导"></a>算法推导</h1><p>对于$m$个样本观察数据${x^{(1)},x^{(2)},x^{(3)}…,x^{(m)}}$，找出样本的模型参数$θ$, 极大化模型分布的对数似然函数如下<br>$$<br>\begin{aligned}<br>θ={\arg\max}_θ \sum_i \log⁡ {P(x^i;θ)}<br>\end{aligned}<br>$$<br>如果我们得到的观察数据有未观察到的隐含数据$z={z^{(1)},z^{(2)},z^{(3)}…,z^{(m)}}$，此时我们的极大化模型分布的对数似然函数如下：<br>$$<br>θ={\arg\max}_θ \sum_i \log {P(x^i;\theta)}={\arg\max}_θ \sum_i \log \sum_z {p(x^i,z^i;\theta)}<br>$$<br>第一步是对极大似然取对数，第二步是对每个样例的每个可能类别$z$求联合分布概率和。<br>首先，我们把分子分母同乘以一个相等的函数（即隐变量$Z$的概率分布$Q_i {(z^{(i)})}$，其概率之和等于1，即$\sum_z Q_i {(z^{(i)})}=1$。其次，利用<strong>Jensen不等式</strong>（即，在凸函数中，有函数的期望大于等于期望的函数，即$E[f(X)]&gt;=f(E[X])$；当$f$是（严格）凹函数当且仅当$-f$是（严格）凸函数，不等号方向反向），我们将上式进行缩放，可得：<br>$$<br>\begin{aligned}<br>\sum_i \log p(x^{(i)};\theta)=\sum_t \log \sum_{z^{(i)}} p(x^{(i)},z^{(i)};\theta) \\<br>=\sum_i \log \sum_{z^{(i)}} Q_i {(z^{(i)})} \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i {(z^{(i)})}} \\<br>\geq \sum_i \sum_{z^{(i)}} Q_i {(z^{(i)})} \log  \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i {(z^{(i)})}}<br>\end{aligned}<br>$$<br>其中$\sum_{z^{(i)}} Q_i {(z^{(i)})} \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i {(z^{(i)})}}$就是$\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i {(z^{(i)})}}$的期望，$log$函数为凹函数（其二次导数为$\frac{−1}{x^2}&lt;0$）。<br>此时如果要满足Jensen不等式的等号，则有：<br>$$<br>\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i {(z^{(i)})}}=c,c为常数<br>$$<br>对该式做个变换，并对所有的z求和，得到<br>$$<br>\sum_z p(x^{(i)},z^{(i)};\theta)=\sum_z Q_i {(z^{(i)})}c<br>$$<br>由于$Q_i {(z^{(i)})}$是一个分布，所以满足$\sum_z Q_i {(z^{(i)})}=1 $<br>可得：<br>$$<br>\sum_z p(x^{(i)},z^{(i)};\theta) = c \\<br>Q_i {(z^{(i)})}=\frac{p(x^{(i)},z^{(i)};\theta)}{c}=<br>\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_z p(x^{(i)},z^{(i)};\theta)}=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} = p(z^{(i)}|x^{(i)};\theta)<br>$$<br>至此，我们推出了在固定参数$θ$后，使下界拉升的$Q_i {(z^{(i)})}$的计算公式就是条件概率，解决了$Q_i {(z^{(i)})}$如何选择的问题。这一步就是E步，建立$L(θ)$的下界。接下来的M步，就是在给定$Q_i {(z^{(i)})}$后，调整$θ$：<br>$$<br>\begin{aligned}<br>{\arg\max}_θ \sum_i \sum_{z^{(i)}} Q_i {(z^{(i)})}  \log<br>\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i {(z^{(i)})}}<br>\end{aligned}<br>$$<br>去掉上式中为常数的部分，则我们需要极大化的对数似然下界为：<br>$$<br>\begin{aligned}<br>{\arg\max}_θ \sum_i \sum_{z^{(i)}} Q_i {(z^{(i)})}  \log<br>{p(x^{(i)},z^{(i)};\theta)}<br>\end{aligned}<br>$$<br>上式也就是我们的EM算法的M步。</p>
<blockquote>
<p><strong>EM算法能保证收敛吗？如果收敛，那么能保证收敛到全局最大值吗？</strong></p>
</blockquote>
<p>要证明EM算法收敛，则我们需要证明我们的 <strong><font color="red">对数似然函数的值在迭代的过程中一直在增大。</font></strong><br>$$<br>\sum_{i=1}^m \log {P(x^i;\theta^{j+1})} \geq \sum_{i=1}^m \log {P(x^i;\theta^{j})}<br>$$<br>由于：<br>$$<br>L(\theta;\theta^{j})=\sum_{i=1}^m \sum_{z^i} P(z^{(i)}|x^i;\theta^j) \log P(x^{(i)},z^{(i)};\theta)<br>$$<br>令：<br>$$<br>H(\theta;\theta^{j})=\sum_{i=1}^m \sum_{z^i} P(z^{(i)}|x^i;\theta^j) \log P(z^{(i)}|x^i;\theta^j)<br>$$<br>上两式相减得到：<br>$$<br>\sum_{i=1}^m \log P(x^i;\theta)=L(\theta;\theta^{j})-H(\theta;\theta^{j})<br>$$<br>在上式中分别取$θ$为$θ^j$和$θ^(j+1)$，并相减得到<br>$$<br>\sum_{i=1}^m \log P(x^i;\theta^{j+1})-\sum_{i=1}^m \log P(x^i;\theta^{j})=[L(\theta^{j+1};\theta^{j})-L(\theta^{j};\theta^{j})]-[H(\theta^{j+1};\theta^{j})-H(\theta^{j};\theta^{j})]<br>$$<br>要证明EM算法的收敛性，我们只需要证明上式的右边是非负的即可。<br>由于$θ^{j+1}$使得$ L(θ,θ^j )$极大，因此有:<br>$$<br>L(\theta^{j+1};\theta^{j})-L(\theta^{j};\theta^{j}) \geq 0<br>$$<br>而对于第二部分，我们有：<br>$$<br>\begin{aligned}<br>H(\theta^{j+1};\theta^{j})-H(\theta^{j};\theta^{j})=&amp; \sum_{i=1}^m \sum_{z^i} P(z^{(i)}|x^i;\theta^j) \log \frac {P(z^{(i)}|x^i;\theta^{j+1}) }{P(z^{(i)}|x^i;\theta^{j}) } \\<br>\leq &amp;\sum_{i=1}^m \log (\sum_{z^i} P(z^{(i)}|x^i;\theta^{j}) \frac{P(z^{(i)}|x^i;\theta^{j+1})}{P(z^{(i)}|x^i;\theta^{j})}) \\<br>=&amp; \sum_{i=1}^m \log (\sum_{z^i} P(z^{(i)}|x^i;\theta^{j+1}))=0<br>\end{aligned}<br>$$<br>其中第2式用到了Jensen不等式，只不过和之前使用相反而已，第3式用到了概率分布累积为1的性质。<br>至此，我们得到了<br>$$<br>\sum_{i=1}^m \log {P(x^i;\theta^{j+1})} - \sum_{i=1}^m \log {P(x^i;\theta^{j})} \geq 0<br>$$<br>证明了EM算法的收敛性。从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标$L(θ,θ^j)$是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。如果我们从算法思想的角度来思考EM算法，我们可以发现我们的算法里已知的是观察数据，未知的是隐含数据和模型参数，在E步，我们所做的事情是固定模型参数的值，优化隐含数据的分布，而在M步，我们所做的事情是固定隐含数据分布，优化模型参数的值。比较下其他的机器学习算法，其实很多算法都有类似的思想。比如SMO算法，坐标轴下降法(Lasso回归算法), 都使用了类似的思想来求解问题。</p>
</div><div class="tags"><a href="/tags/algorithms/">algorithms</a></div><div class="post-nav"><a class="pre" href="/2019/05/04/聚类算法一览/">聚类算法一览</a><a class="next" href="/2019/05/03/降维算法一览/">降维算法一览</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/algorithms/" style="font-size: 15px;">algorithms</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/Algorithm-practice/" style="font-size: 15px;">Algorithm practice</a> <a href="/tags/Books-Note/" style="font-size: 15px;">Books Note</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/概率图模型/">概率图模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/06/百面机器学习/">百面机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/聚类算法一览/">聚类算法一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/EM算法/">EM算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/降维算法一览/">降维算法一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/29/xgboost-lightgbm调参指南/">xgboost&lightgbm调参指南</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/24/集成学习/">集成学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/机器学习基础问题/">机器学习基础概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/决策树模型/">决策树模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/22/支持向量机模型/">支持向量机模型</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="https://github.com/helloJamest" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Jamest.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>