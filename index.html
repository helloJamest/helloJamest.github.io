<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Jamest</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Jamest</h1><a id="logo" href="/.">Jamest</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title"><a href="/2019/05/03/降维算法一览/">降维算法一览</a></h1><div class="post-meta">2019-05-03</div><div class="post-content"><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line">在机器学习中经常会碰到一些高维的数据集，而在高维数据情形下会出现数据样本稀疏，距离计算等困难，这类问题是所有机器学习方法共同面临的严重问题，称之为 **&lt;font color=red&gt;“ 维度灾难 ”&lt;/font&gt;**。另外在高维特征中容易出现特征之间的线性相关，这也就意味着有的特征是冗余存在的。基于这些问题，降维思想就出现了。</span><br><span class="line">降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。通过降维，可以方便数据可视化+数据分析+数据压缩+数据提取等。</span><br><span class="line"></span><br><span class="line"># 降维方法架构</span><br><span class="line">降维方法主要包括属性选择和映射方法。属性选择即特征选择，实际上，特征选择和传统的特征降维也有一定的区别。特征降维本质上是从一个维度空间映射到另一个维度空间，特征的多少并没有减少，当然在映射的过程中特征值也会相应的变化。特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后不改变值，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征。</span><br><span class="line">这里我们主要讲述映射方法，对于特征选择，我们会在后面进行详细的阐述。</span><br><span class="line"></span><br><span class="line">- 属性选择：</span><br><span class="line">  过滤法；包装法；嵌入法；</span><br><span class="line">- 映射方法</span><br><span class="line">  - 线性映射方法：PCA、LDA、SVD分解等</span><br><span class="line">  - 非线性映射方法：</span><br><span class="line">    - 核方法：KPCA、KFDA等</span><br><span class="line">    - 二维化：</span><br><span class="line">    - 流形学习：ISOMap、LLE、LPP等。</span><br><span class="line">- 其他方法：神经网络和聚类</span><br><span class="line"></span><br><span class="line"># PCA</span><br><span class="line">PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。 **&lt;font color=red&gt;PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，提取数据的主要特征分量，常用于高维数据的降维。&lt;/font&gt;**</span><br><span class="line">PCA有两种通俗易懂的解释：(<span class="number">1</span>)最大方差理论；(<span class="number">2</span>)最小平方误差。下面主要从最大方差理论出发，推导出表达式</span><br><span class="line">## 最大方差理论</span><br><span class="line">PCA的目标可认为是最大化投影方差，也就是让数据在主轴上投影的方差最大。</span><br><span class="line">对于给定的一组数据点$\&#123;v_1,v_2,…,v_n\&#125;$,其中所有向量均为列向量，对其进行中心化，表示为$\&#123;x_1,x_2,…,x_n\&#125;$。可得向量$x_i$在$w$（单位方向向量）上的投影坐标可以表示为$(x_i,w)=x_i^<span class="built_in">T</span> w$，因此我们的目标是找到一个投影方向$w$，使得$\&#123;x_1,x_2,…,x_n\&#125;$在$w$上的投影方差尽可能大。因为投影之后的均值为<span class="number">0</span>，因此方差可以表示为：</span><br><span class="line">$$</span><br><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">D(x)=&amp; \frac&#123;<span class="number">1</span>&#125;&#123;<span class="built_in">n</span>&#125; \sum_&#123;i=<span class="number">1</span>&#125;^<span class="built_in">n</span> (x_i^<span class="built_in">T</span> w)^<span class="built_in">T</span> x_i^<span class="built_in">T</span> w \\</span><br><span class="line">=&amp; \frac&#123;<span class="number">1</span>&#125;&#123;<span class="built_in">n</span>&#125; \sum_&#123;i=<span class="number">1</span>&#125;^<span class="built_in">n</span> w^<span class="built_in">T</span> x_i &#123;x_i&#125;^<span class="built_in">T</span> w \\</span><br><span class="line">=&amp; w^<span class="built_in">T</span> (\frac&#123;<span class="number">1</span>&#125;&#123;<span class="built_in">n</span>&#125; \sum_&#123;i=<span class="number">1</span>&#125;^<span class="built_in">n</span> x_i x_i^<span class="built_in">T</span>)w</span><br><span class="line">\end&#123;aligned&#125;</span><br><span class="line">$$</span><br><span class="line">其中，$\frac&#123;<span class="number">1</span>&#125;&#123;<span class="built_in">n</span>&#125; \sum_&#123;i=<span class="number">1</span>&#125;^<span class="built_in">n</span> x_i x_i^<span class="built_in">T</span>$为样本协方差矩阵，令为$∑$，另外由于$w$是单位方向向量，即$w^<span class="built_in">T</span> w=<span class="number">1</span>$，因此目标可写作：</span><br><span class="line">$$</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">\max\&#123;w^<span class="built_in">T</span>\sum w\&#125; \\</span><br><span class="line">s.t. w^<span class="built_in">T</span> w=<span class="number">1</span></span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">$$</span><br><span class="line">引入拉格朗日乘子，对$w$求导令其为<span class="number">0</span>，可以推出$∑w=λw$，此时</span><br><span class="line">$$</span><br><span class="line">D(x)=w^<span class="built_in">T</span>∑w=λw^<span class="built_in">T</span> w=λ</span><br><span class="line">$$</span><br><span class="line">即，x投影后方差即协方差矩阵的特征值，最佳投影方向就是最大特征值对应的特征向量。次佳投影方向是第二大特征值对应的特征向量，以此类推，可得以下PCA的求解方法。</span><br><span class="line"><span class="number">1</span>. 对样本数据进行 **中心化处理**。</span><br><span class="line"><span class="number">2</span>. 求样本 **协方差矩阵**</span><br><span class="line"><span class="number">3</span>. 对协方差矩阵进行 **特征值分解**，将特征值从大到小排列</span><br><span class="line"><span class="number">4</span>. 取特征值前d大对应的特征向量$w_1,w_2,…,w_d$,通过以下映射将<span class="built_in">n</span>维样本映射到d维</span><br><span class="line">$$</span><br><span class="line">&#123;x_i&#125;'=</span><br><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">&#123;w_1&#125;^Tx_i\\</span><br><span class="line"> \vdots \\</span><br><span class="line">&#123;w_d&#125;^Tx_i\\</span><br><span class="line">\end&#123;bmatrix&#125;</span><br><span class="line">$$</span><br><span class="line">定义降维后信息占比为：</span><br><span class="line">$$</span><br><span class="line">\eta=\sqrt&#123; \frac&#123;\sum_&#123;i=<span class="number">1</span>&#125;^d&#123;\lambda_i&#125;^<span class="number">2</span>&#125;&#123; \sum_&#123;i=<span class="number">1</span>&#125;^<span class="built_in">n</span>&#123;\lambda_i&#125;^<span class="number">2</span> &#125; &#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">另外，由于得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法。</span><br><span class="line">现实场景下，一般使用SVD，它有两个好处：</span><br><span class="line"><span class="number">1</span>) 有一些SVD的实现算法可以先不求出协方差矩阵也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解而是通过SVD来完成，这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是特征值分解。</span><br><span class="line"><span class="number">2</span>) 注意到PCA仅仅使用了我们SVD的左奇异矩阵，没有使用到右奇异值矩阵，那么右奇异值矩阵有什么用？左奇异矩阵可以用于对行数的压缩；右奇异矩阵可以用于对列(即特征维度)的压缩。这就是我们用SVD分解协方差矩阵实现PCA可以得到两个方向的PCA降维(即行和列两个方向)。</span><br><span class="line"></span><br><span class="line">PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</span><br><span class="line">最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</span><br><span class="line"></span><br><span class="line"># LDA</span><br><span class="line">线性判别分析（Linear Discriminant Alalysis,LDA）是一种有监督学习算法，同时经常被用来对数据进行降维。</span><br><span class="line">**&lt;font color=red&gt;LDA的中心思想是最大化类间距离和最小化类内距离，即将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。&lt;/font&gt;**</span><br><span class="line">设有<span class="symbol">C1</span>、<span class="symbol">C2</span>两个类别的样本，两类的均值分别为$μ_1=\frac&#123;<span class="number">1</span>&#125;&#123;<span class="built_in">N</span>&#125; \sum_&#123;x∈C_1&#125; x$, $μ_2=\frac&#123;<span class="number">1</span>&#125;&#123;<span class="built_in">N</span>&#125; \sum_&#123;x∈C_2&#125; x$，则两类中心在w方向上的投影向量为$w^<span class="built_in">T</span> μ_1$，$w^<span class="built_in">T</span> μ_2$，因此类间距离为$||w^<span class="built_in">T</span> (μ_1-μ_2 )||_2^<span class="number">2</span>$；我们将类内方差定义为各个类分别的方差之和，将目标函数定义为类间距离和类内距离的比值，于是我们最大化的目标为：</span><br><span class="line">$$</span><br><span class="line">\max_w⁡ J(w)=\frac&#123;||w^<span class="built_in">T</span> (μ_1-μ_2 )||_2^<span class="number">2</span>&#125;&#123;D_1+D_2&#125;</span><br><span class="line">$$</span><br><span class="line">其中$w$为单位向量，$D_1$，$D_2$分别表示两类投影后的方差</span><br><span class="line">$$</span><br><span class="line">D_1=\sum_&#123;x∈C_1&#125;(w^<span class="built_in">T</span> x-w^<span class="built_in">T</span> μ_1)^<span class="number">2</span> =∑_&#123;x∈C_1&#125;w^<span class="built_in">T</span> (x-μ_1)(x-μ_1)^<span class="built_in">T</span> w</span><br><span class="line">$$</span><br><span class="line">$$</span><br><span class="line">D_2=∑_&#123;x∈C_2&#125;w^<span class="built_in">T</span> (x-μ_2)(x-μ_2)^<span class="built_in">T</span> w</span><br><span class="line">$$</span><br><span class="line">因此，$J(w)$可写成</span><br><span class="line">$$</span><br><span class="line">J(w)=\frac&#123;w^<span class="built_in">T</span> (μ_1-μ_2)(μ_1-μ_2)^<span class="built_in">T</span> w&#125;&#123;∑_&#123;x∈C_i&#125; &#123;w^<span class="built_in">T</span> (x-μ_i)(x-μ_i)^<span class="built_in">T</span> w&#125; &#125;</span><br><span class="line">$$</span><br><span class="line">定义类间散度矩阵$S_B=(μ_1-μ_2)(μ_1-μ_2)^<span class="built_in">T</span>$, 类内散度矩阵$S_w=∑_&#123;x∈C_i&#125;(x-μ_i)(x-μ_i)^<span class="built_in">T</span> $，得</span><br><span class="line">$$</span><br><span class="line">J(w)=\frac&#123;w^<span class="built_in">T</span> S_B w&#125;&#123;&#123;w^<span class="built_in">T</span> S_w w&#125; &#125;</span><br><span class="line">$$</span><br><span class="line">于是，可得</span><br><span class="line">$$</span><br><span class="line">S_B w=λS_w w</span><br><span class="line">$$</span><br><span class="line">即</span><br><span class="line">$$</span><br><span class="line">S_w^&#123;-<span class="number">1</span>&#125; S_B w=λw</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">即求矩阵特征向量问题。$J(w)$对应了矩阵$S_w^&#123;-<span class="number">1</span>&#125; S_B$最大的特征值，而投影方向是这个特征值对应的特征向量。</span><br><span class="line">LDA在降维过程中可以使用类别的先验知识经验，在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。</span><br><span class="line">## PCA与LDA异同？</span><br><span class="line">相同点</span><br><span class="line"><span class="number">1</span>）两者均可以对数据进行降维。</span><br><span class="line"><span class="number">2</span>）两者在降维时均使用了矩阵特征分解的思想。</span><br><span class="line"><span class="number">3</span>）两者都假设数据符合高斯分布。</span><br><span class="line">不同点</span><br><span class="line"><span class="number">1</span>）LDA是有监督的降维方法，而PCA是无监督的降维方法</span><br><span class="line"><span class="number">2</span>）LDA降维最多降到类别数k-<span class="number">1</span>的维数，而PCA没有这个限制。</span><br><span class="line"><span class="number">3</span>）LDA除了可以用于降维，还可以用于分类。</span><br><span class="line"><span class="number">4</span>）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。</span><br><span class="line"></span><br><span class="line"># 局部线性嵌入 （LLE）</span><br><span class="line">Locally linear embedding（LLE）是一种非线性降维算法，它能够使降维后的数据较好地保持原有流形结构 。LLE可以说是流形学习方法最经典的工作之一。很多后续的流形学习、降维方法都与LLE有密切联系。</span><br><span class="line">&gt; 和传统的PCA，LDA等关注样本方差的降维方法相比，LLE关注于降维时保持样本局部的线性特征，由于LLE在降维时保持了样本的局部特征，它广泛的用于图像图像识别，高维数据可视化等领域。</span><br><span class="line"></span><br><span class="line">流形学习是一大类基于流形的框架。数学意义上的流形比较抽象，不过我们可以认为LLE中的流形是一个不闭合的曲面。这个流形曲面数据分布比较均匀，且比较稠密。基于流行的降维算法就是将流形从高维到低维的降维过程，在降维的过程中我们希望流形在高维的一些特征可以得到保留。</span><br><span class="line">LLE算法认为每一个数据点都可以由其近邻点的线性加权组合构造得到。算法的主要步骤分为三步：(<span class="number">1</span>)寻找每个样本点的k个近邻点；（<span class="number">2</span>）由每个 样本点的近邻点计算出该样本点的局部重建权值矩阵；（<span class="number">3</span>）由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值。</span><br><span class="line">LLE算法很简单高效，但是却有一些问题，比如如果近邻数k大于输入数据的维度时，我们的权重系数矩阵不是满秩的。为了解决这样类似的问题，有一些LLE的变种产生出来。比如：Modified Locally Linear Embedding(MLLE)和Hessian Based LLE(HLLE)。对于HLLE，它不是考虑保持局部的线性关系，而是保持局部的Hessian矩阵的二次型的关系。而对于MLLE，它对搜索到的最近邻的权重进行了度量，我们一般都是找距离最近的k个最近邻就可以了，而MLLE在找距离最近的k个最近邻的同时要考虑近邻的分布权重，它希望找到的近邻的分布权重尽量在样本的各个方向，而不是集中在一侧。</span><br><span class="line">另一个比较好的LLE的变种是Local tangent space alignment(LTSA)，它希望保持数据集局部的几何关系，在降维后希望局部的几何关系得以保持，同时利用了局部几何到整体性质过渡的技巧。</span><br><span class="line">这些算法原理都是基于LLE，基本都是在LLE这三步过程中寻求优化的方法。</span><br><span class="line">## 优缺点</span><br><span class="line">LLE是广泛使用的图形图像降维方法，它实现简单，但是对数据的流形分布特征有严格的要求。比如不能是闭合流形，不能是稀疏的数据集，不能是分布不均匀的数据集等等，这限制了它的应用。下面总结下LLE算法的优缺点。</span><br><span class="line">LLE算法的主要优点有：</span><br><span class="line"><span class="number">1</span>）可以学习任意维的局部线性的低维流形</span><br><span class="line"><span class="number">2</span>）算法归结为稀疏矩阵特征分解，计算复杂度相对较小，实现容易。</span><br><span class="line"></span><br><span class="line">LLE算法的主要缺点有：</span><br><span class="line"><span class="number">1</span>）算法所学习的流形只能是不闭合的，且样本集是稠密均匀的。</span><br><span class="line"><span class="number">2</span>）算法对最近邻样本数的选择敏感，不同的最近邻数对最后的降维结果有很大影响。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># <span class="built_in">t</span>-SNE</span><br><span class="line"><span class="built_in">t</span>-SNE，即<span class="built_in">t</span>-分布领域嵌入算法，读作“Tee-Snee”，是一种用于挖掘高维数据的非线性降维算法。</span><br><span class="line">算法出发点是：在高维空间相似的数据点，映射到低维空间距离也是相似的。常规的做法是用欧式距离表示这种相似性，而SNE把这种距离关系转换为一种条件概率来表示相似性。</span><br><span class="line">SNE主要想法是，将高维分布点的距离，用条件概率来表示相似性，同时低维分布的点也这样表示。只要二者的条件概率非常接近（用相对熵来训练，所以需要label），那就说明高维分布的点已经映射到低维分布上了。难点是：高维距离较近的点，比较方便聚在一起，但是高维距离较远的点，却比较难在低维拉开距离。其次，训练的时间也比较长。</span><br><span class="line"><span class="built_in">t</span>-SNE算法把SNE变为对称SNE，提高了计算效率，效果稍有提升；并且在低维空间中采用了<span class="built_in">t</span>分布代替原来的高斯分布，解决了拥挤问题，优化了SNE过于关注局部特征忽略全局特征的问题</span><br><span class="line">理论部分可参考：[从SNE到<span class="built_in">t</span>-SNE再到LargeVis](ht<span class="symbol">tp:</span>//bindog.github.io/blog/<span class="number">2016</span>/<span class="number">06</span>/<span class="number">04</span>/from-sne-to-tsne-to-largevis/)</span><br><span class="line">在实际应用中，<span class="built_in">t</span>-SNE很少用于降维，主要用于可视化，可能的原因有以下几方面：</span><br><span class="line">- 当我们发现数据需要降维时，一般是特征间存在高度的线性相关性，此时我们一般使用线性降维算法，比如PCA。即使是特征之间存在非线性相关，也不会先使用非线性降维算法降维之后再搭配一个线性的模型，而是直接使用非线性模型；</span><br><span class="line">- 一般 <span class="built_in">t</span>-SNE 都将数据降到 <span class="number">2</span> 维或者 <span class="number">3</span> 维进行可视化，但是数据降维降的维度一般会大一些，比如需要降到 <span class="number">20</span> 维，<span class="built_in">t</span>-SNE 算法使用自由度为 <span class="number">1</span> 的 <span class="built_in">t</span> 分布很难做到好的效果；</span><br><span class="line">- <span class="built_in">t</span>-SNE 算法的计算复杂度很高，另外它的目标函数非凸，可能会得到局部最优解；</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt; <span class="built_in">t</span>-SNE在高维空间中采用的高斯核心函数定义了数据的局部和全局结构之间的软边界。对于高斯的标准偏差而言彼此临近的数据点对，对它们的间隔建模的重要性几乎与那些间隔的大小无关。 此外，<span class="built_in">t</span>-SNE基于数据的局部密度（通过强制每个条件概率分布具有相同的困惑度）分别确定每个数据点的局部邻域大小。 这是因为算法定义了数据的局部和全局结构之间的软边界。 与其他非线性降维算法不同，它的性能优于其它任何一个算法。</span><br></pre></td></tr></table></figure></div><p class="readmore"><a href="/2019/05/03/降维算法一览/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/29/xgboost-lightgbm调参指南/">xgboost&amp;lightgbm调参指南</a></h1><div class="post-meta">2019-04-29</div><div class="post-content"><p>本文重点阐述了xgboost和lightgbm的主要参数和调参技巧，其理论部分可见<a href="https://hellojamest.github.io/2019/04/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">集成学习</a>,以下内容主要来自<a href="https://xgboost.readthedocs.io/en/latest/parameter.html" target="_blank" rel="noopener">xgboost</a>和<a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" target="_blank" rel="noopener">LightGBM</a>的官方文档。</p></div><p class="readmore"><a href="/2019/04/29/xgboost-lightgbm调参指南/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/24/集成学习/">集成学习</a></h1><div class="post-meta">2019-04-24</div><div class="post-content"><p>集成学习的目的是通过结合多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性。<br>目前主流方法有三种：<br>1.Boosting方法：包括Adaboost，GBDT, XGBoost等<br>2.Bagging方法：典型的是Random Forest<br>3.Stacking算法<br></p></div><p class="readmore"><a href="/2019/04/24/集成学习/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/23/机器学习基础问题/">机器学习基础概念</a></h1><div class="post-meta">2019-04-23</div><div class="post-content"><p>记录一些常见的机器学习基础概念。<br></p></div><p class="readmore"><a href="/2019/04/23/机器学习基础问题/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/23/决策树模型/">决策树模型</a></h1><div class="post-meta">2019-04-23</div><div class="post-content"><p>决策树的目标是从一组样本数据中，根据不同的特征和属性，建立一棵树形的分类结构。<br><strong>决策树的学习本质上是从训练集中归纳出一组分类规则，得到与数据集矛盾较小的决策树，同时具有很好的泛化能力。<font color="red">决策树学习的损失函数通常是正则化的极大似然函数</font></strong>，通常采用启发式方法，近似求解这一最优化问题。<br></p></div><p class="readmore"><a href="/2019/04/23/决策树模型/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/22/支持向量机模型/">支持向量机模型</a></h1><div class="post-meta">2019-04-22</div><div class="post-content"><p>支持向量机模型(SVM)是一个二分类模型，基本思想是<strong>求解能够正确划分训练数据集并且几何间隔最大的分离超平面</strong>，其学习策略便是间隔最大化，最终化为一个凸二次规划问题的求解。<br>SVM可分为线性可分支持向量机、线性支持向量机和非线性支持向量机。</p></div><p class="readmore"><a href="/2019/04/22/支持向量机模型/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/21/逻辑回归模型/">逻辑回归模型</a></h1><div class="post-meta">2019-04-21</div><div class="post-content"><p>逻辑回归模型是针对线性可分问题的一种易于实现而且性能优异的分类模型。<br><strong>它假设数据服从<font color="red">伯努利分布</font>,通过<font color="red">极大化似然函数</font>的方法，运用<font color="red">梯度下降法</font>来求解参数，来达到将数据二分类的目的。</strong></p></div><p class="readmore"><a href="/2019/04/21/逻辑回归模型/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/19/最大熵模型/">最大熵模型</a></h1><div class="post-meta">2019-04-19</div><div class="post-content"><p>最大熵模型是指在满足约束条件的模型集合中选取熵最大的模型，即不确定性最大的模型。<br></p></div><p class="readmore"><a href="/2019/04/19/最大熵模型/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/22/hello-world/">Hello World</a></h1><div class="post-meta">2019-01-22</div><div class="post-content"><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p></div><p class="readmore"><a href="/2019/01/22/hello-world/">Mehr lesen</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/11/11/sklearn-ml-tutorial/">sklearn &amp; ml tutorial</a></h1><div class="post-meta">2018-11-11</div><div class="post-content"><h1 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h1><p>pd.scatter_matrix(pd.DataFrame(X_train),c=y_train_name,figsize=(15,15),marker=’o’,hist_kwds={‘bins’:20},s=60,alpha=.8,cmap=mglearn.cm3)#绘制散点图矩阵（pair plot），两两查看所有的特征</p></div><p class="readmore"><a href="/2018/11/11/sklearn-ml-tutorial/">Mehr lesen</a></p></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/algorithms/" style="font-size: 15px;">algorithms</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/Algorithm-practice/" style="font-size: 15px;">Algorithm practice</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/降维算法一览/">降维算法一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/29/xgboost-lightgbm调参指南/">xgboost&lightgbm调参指南</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/24/集成学习/">集成学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/机器学习基础问题/">机器学习基础概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/决策树模型/">决策树模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/22/支持向量机模型/">支持向量机模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/21/逻辑回归模型/">逻辑回归模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/19/最大熵模型/">最大熵模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/22/hello-world/">Hello World</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/11/sklearn-ml-tutorial/">sklearn & ml tutorial</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="https://github.com/helloJamest" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Jamest.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>